# SUMMARY
Eduardo discusses the third chapter of "AI Engineering" by Chipan, focusing on evaluation methodologies for AI applications and how to measure their effectiveness.

# IDEAS:
- Evaluation methodologies are crucial for measuring AI application effectiveness.
- AI applications should serve a clear purpose for users.
- Non-deterministic responses from AI complicate performance measurement.
- Techniques like A/B testing and multi-arm bandits can assess user outcomes.
- Functional correctness checks if generated outputs meet predefined tests.
- Comparison tests evaluate outputs against a dataset of golden answers.
- Human evaluators provide better correlation to perceived correctness than automated methods.
- AI can serve as a judge, but its reliability depends on transparency.
- Perplexity measures how well a model captures underlying data.
- Higher perplexity indicates greater uncertainty in predictions.
- Context length affects perplexity; more context leads to lower perplexity.
- Structured data reduces decision-making complexity for models.
- The importance of understanding the evaluation process for developers.
- Subtle changes in AI performance can be hard to detect.
- User feedback is essential for evaluating AI effectiveness.
- Consistency in evaluation methods is critical for accurate assessments.

# QUOTES:
- "Every AI app needs to serve a purpose and we need to measure if it is achieving that purpose."
- "Due to the nature of non-deterministic responses, it's hard to pinpoint if it's actually improving or not."
- "Functional correctness... does it pass the test some test that I set up for that code?"
- "Human in the loop can help on this."
- "AI as a judge... has high agreement with human evaluators."
- "You need to have control over the evaluation for your use case."
- "The more uncertainty the model has in predicting what comes next, the higher the perplexity."
- "The higher the context length, the lower the perplexity."
- "If you give structured data... the model needs to make less decisions on what to do next."
- "It's hard to make a judgment on what you should do as a developer based on AI as a judge."

# FACTS:
- A/B testing and multi-arm bandits are techniques used for evaluating AI applications.
- Functional correctness is a method to ensure generated outputs meet specific criteria.
- Perplexity is a common metric used to evaluate language models.
- The complexity of vocabulary can increase perplexity in models.
- Human evaluators often provide better insights than automated evaluations.
- AI judges can be costly and inconsistent in their assessments.
- The evaluation of AI applications involves multiple decision points.
- Non-deterministic responses from AI can lead to challenges in performance measurement.
- The evaluation process can be influenced by changes in model behavior or evaluator standards.

# REFERENCES:
- "AI Engineering" by Chipan
- Techniques like A/B testing and multi-arm bandits
- Functional correctness tests
- Comparison tests against datasets of golden answers
- Perplexity as a metric for language models

# RECOMMENDATIONS:
- Implement A/B testing to measure user outcomes effectively.
- Use functional correctness checks during development phases.
- Incorporate human evaluators in the assessment process for better accuracy.
- Maintain transparency when using AI as a judge for evaluations.
- Regularly review and adjust evaluation methodologies based on user feedback.
- Understand the implications of perplexity in model performance assessments.
- Provide ample context to models to reduce perplexity and improve predictions.
- Explore structured data formats to simplify model decision-making processes.
