hey everyone uh my name is Eduardo and this is a recording 
of our reading-club uh at gitlab uh the AI uh Reading Club 
where we are reading through the-book AI engineering and 
today by uh chipan and today we're talking about the-third 
chapter just as a note this Reading Club already took place 
uh last-week but I forgot to hit record it was a great 
session uh lots of-uh questions lot of lots of discussion 
uh-but since I cannot upload the recording because there's 
no recording I might as-well uh go through the the the some 
of the the the summary of what was-discussed so the third 
chapter uh is called evaluation methodology which is-about 
how will you measure your llm uh or rather how you measure 
your AI app-how do you know that if it's working or not um 
this is a more theoretical uh uh-chapter which is quite 
good uh but overall you have three decision points-on on 
evaluation uh just at a high level and-what you're trying 
to do in the end is deploy an app that helps users or 
your-internal uh processes achieve some goal right this is 
this this is why we're-doing this kind of thing we have to 
help uh our final consumer to uh achieve uh-something so 
and for this we choose a stock llm so a foundational 
model-whether it's a y or um I don't know open AI models or 
whatever and then we-augment this llm with our prompt with 
the system prompt or with uh-retrieval uh context or with 
additional context Tex uh so at each of these-points we can 
measure whether we are doing a good job or not and maybe 
make-better decisions stream better uh help the user 
achieve their goals so how do-we go about this what kind of 
techniques we can use about uh for for evaluating-at these 
levels so I'll start by the end so how do we evaluate the 
final app and-then the augmentation llm and then llm um 
measuring first of all how do we-measure if the deploy if I 
the the the user is achieving the like the F how do-do you 
measure the final goal um like I said every AI app needs-to 
serve a purpose and we need to measure if it is achieving 
that purpose-and due to the nature of non-deterministic 
responses we have-from AI apps uh in the case in this case 
either-because like temperature was high generate a lot of 
responses or rral uh-returns different responses it's hard 
toing point deterministic if it's-actually improving or not 
um so you need to do this by-measuring the outcome either 
by comparing multiple versions with AB-tests uh either by 
using multi arm Bandits or direct your ux research-get in 
contact with the users and see if it's improving or not 
sometimes this is-hard because the changes might be very 
subtle uh and a user cannot really Pino-which one is better 
if we actually improve their response or not uh any Tas-can 
help in that scenario uh but it's always there's no super 
bullet for this-problem uh and this is a larger uh 
discussion that measuring the impact of-of a policy in 
itself can can Le can have its own uh entire set of 
reading-clubs but this is how that that's some of the 
techniques that could be used the-next step on the Chain 
would be uh on the Chain backwards I mean it's-like the 
first uh is augmenting uh the llm how do we measure if 
the-modifications that we did to the llm are good to the 
use case this already the-final one measures the outcome 
but this one-is before we put this in production how do we 
measure if the changes that we are-making like the guidance 
that we're adding to the llm is actually doing its-job so 
the author puts on a few techniques uh which are 
quite-interesting so the first one is functional 
correctness which I think-it's quite awesome like really is 
okay if I-generate a um a code uh response for example code 
completion does it pass the-test some test that I set up 
for that code so as I am coding as I am doing the-task I 
have a test that actually tests if that generation is 
correct or-not uh another technique is comparison so you 
generate multiple comparisons-either from the stock llm 
without adding-augmentation or a previous version um or and 
then test uh how do-you how does this new response 
generated compared to the other ones the downside-is that 
these require data sets to be built uh no wait comparison 
test is uh-you have a data set and then you compare compare 
the outputs of the llm against-that that data set of golden 
answers uh so for example uh you can check how-close they 
are to each other lexically or-semantically um and measure 
the impact but this it's good-that it's automated but at 
the same time it's it doesn't always correlate to 
any-improv in human perception of correctness so you can 
use like a-semantical uh approach to check if two answers 
are similar but that doesn't-always work so that's why 
human in the loop uh can can help on this so you have-you 
ask actual people to evaluate two answers and see if they 
rank one higher-than the other um compare different model 
and so on so forth uh and that-correlates better to what we 
want to aim in the-end um and the last one the last 
technique-is AI as a judge where you ask an AI model to 
test whether that answer that-was given by your your 
modifications your augmented llm actually solve the-problem 
or actually achieve uh what you're trying to achieve-the 
plus side is that is it has high agreement with human 
evaluators so some-papers have analyzed this this AI as a 
judge methodology-and if in general if AI judge say it's 
good a human will all say it's good but-it can also be 
costly so you have to make a request to a Ani human the 
loop-is also costly because you need to ask ask a human to 
do the-to do the evaluation but again it can be costly it 
can be inconsistent so if-something changes on the model 
it's a little bit complicated to-identify uh where does 
that change happen like if something changes in 
the-valuation and it can also be hard to reproduce it's 
non-deterministic uh so for example if you see a drop 
in-score is it because in the valuations is it because the 
model-that you changed is actually doing worse or is this 
just that the judge is being-more um I don't know uh more 
tough on its evaluations so-it's it's it's hard to make a 
judgment on what you should do as developer based-on AI as 
a judge sometimes and the author also puts uh a very 
important-warning is to not trust any AI judge if you can't 
see the model-and the prompt used for the judge because you 
need to have control over-the evaluation for your use case 
otherwise it's you cannot know what will-be your actions so 
like I said if some if the score drops or improves you 
don't-know if it's because the model actually improved or 
if it's because the judge-has changed his behavior so 
that's pretty it's pretty important to to be-consistent 
with that and finally the first step of the-of the chain 
and I think this is the most uh now you're getting from the 
end-to the beginning you get more and more theoretical so 
the end is a lot user-related and here is a lot more 
academical and I won't try to explain-here uh each one of 
this this uh this uh terms because there are a lot of 
uh-there are a lot of videos on the internet that that 
explain them quite-well so the first three terms they they 
they try to-measure how well the model uh the llm captures 
the underlying data it was-trained on um so the 
probabilities of of the text that that were fed into it-uh 
but perplexity is a metric that's been used quite often uh 
for for the for-different models-and Bruno uh another 
colleague a Bruno put it very well that is basically-how 
perplex the model is to the input is being given so what's 
the likely so it-sees some some production and how How 
likely is the next next input to be next-output to be given 
um so I like how the author also-give some interpretations 
or understandings-int intuitions you can drive from 
perplexity so the more uncertainty the-model has in 
predicting what comes next the higher the perplexity 
that-means the higher the context length the lower the 
perplexity-because you are given a lot of context uh the 
same way that if you give-structure data like for example 
HTML or a coding language you can expect some-structure so 
the the the the model needs to make less decisions on on 
what to do-next but also how the the vocabulary increases 
the complexity because there-are more things to be the for 
the motel to choose-from um so that was it that was my 
review of this chapter uh again I'm-sorry for not actually 
recording the discussion there were a lot of different-uh 
points that was very interesting uh I'll be sure to record 
next one but-thank you for watching
