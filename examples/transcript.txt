hey hey everyone and uh welcome uh fourth session uh-today 
and this was a long chapter to read uh-but yeah so for 
posteriority Ione uh my name is Eduardo and this is the 
fourth-uh sync on AI engineering uh reading club uh or 
rather in the AI reading club-in general but more 
specifically on on the AI-engineering book and this is 
chapter 4 which was uh which is evaluate AI-systems it was 
a very interesting chapter and it was very difficult 
to-summarize this one has anyone read the book okay yeah oh 
wow more people than-expected so that's awesome uh I tried 
to summarize it but there was a lot of-information uh I 
this I created the the quick slideshow and I'll will go 
quickly-through it but does anybody have any 
comments-before I start or would you rather leave for later 
all right-so uh yeah so the tldr of this magic is this uh 
this chapter is mostly on the-second uh step of evaluation 
is looking at the modifications on top-of delm so you have 
the you have a model and you're putting stuff on top how 
to-identify if your uh model and your system is actually um 
good for your use-cases and this is a priority before you 
put it on uh production I this is a few-of the quots that I 
found interesting on this chapter uh I'll will not read 
them-specifically if you want to read them afterwards uh 
otherwise we lose but one-of the most important is you 
don't really care which model is the best you-just care 
about your use case like all of these metrics all of these 
benchmarks-that we see online they are at best guiding 
guidance on what is good and-like all of this public bench 
marks that companies are competing for they are-best like 
some guidance on what not to use but the model that we're 
going to-use like that would fit better uh for our use case 
would just be decided after-we evaluate uh against our use 
case uh so here the althor shows a bit-of the process of 
filtering and evaluating uh these models so the first-one 
is rule out models that don't fit your attributes like hard 
attributes for-example privacy if you need or data lineage 
if you really need uh the model-to follow a specific data 
lineage or not used use uh private data this can-already 
roll out a bunch of models or if you need a license that's 
permissive-that could roll out a bunch of models uh with 
that you have a few models uh that-might be available to 
you and then with this you can check their 
public-evaluations Sometimes some benchmarks don't are not 
run for some models and-you have to run yourself but this 
will give you a set of models that you can-test you can 
test out on your task that you're trying to achieve-uh and 
that will give a model that will test-lateration the 
chapter this book for me and custom model team or rather 
that we-are working with self-hosted model had a very uh 
interesting discussion on-whether hosting models and are 
using API when would customer or when would we use-a 
self-hosted versus a hosted one um so C questions like data 
privacy and data-lineage customers that do care about more 
about those which are most of our-like we have a large 
chunk of our customers-that uh give preference to that 
given our-self-deployment so they would then to self host 
models performance usually-hosted models are better 
performance uh because of the incentives like if you-can 
monetize something probably they will models might have 
different-functionality costs but on a self host you have 
full control of the model so if-the API changes you can 
still uh you are not acceptable for example for some 
API-changes on the vendor other point is on device 
deployment on this case they're-talking more the alter 
talks more about like on mobile devices or something 
like-that like Edge but the use case is very similar to 
ours like we want models-should be hosted on our customers 
uh environments so that they-have full privacy over the 
data so this is this was very like there's a table-that I 
would suggest looking at I will probably print and have it 
somewhere on-my wall cuz uh it allows to guide our 
customers on what is best for-them uh but the most 
important and the most uh for me of this chapter uh 
was-about designing the evaluation pipeline it was very 
interesting same it was-almost like a cookbook on how to 
evaluate and how to create an evolve a-pipeline so there 
were three steps one is evaluate all of first uh Define 
how-to evaluate all of the components and I think this is 
very relevant to uh to how-we were discussing with global 
search and rag evaluation it's not only useful-to validate 
the final step like did the user achieve the task but what 
is the-contribution of each of the like decision points 
within this process-for example I have a system prompt here 
where the user makes a question the laam-fetches two R 
gives an answer then the user does another question with 
another-answer and then user does no more question so first 
of all did this whole-process uh achieve the user uh task 
like did it help how can we-measure that what how do we 
evaluate that but how does each one of the rags-contribute 
to this so if we didn't have this R suppose that this 
wasn't possible-would the users still have achieved the 
results or is it just because of the-model that that's good 
enough or did we want to know that because if we can if-we 
can avoid using rag um and wasting resources that's-better 
it might even cause make it worse by causing hallucinations 
depending on-the situation uh by introducing bad rag uh you 
can actually uh create a problem-so measure latency across 
a bit like resource usage energy usage across this-whole 
thing and this an example but how do we measure this at 
scale how do we-measure evaluate this across all of of of 
our pipeline we'll probably need to-build uh checkpoints 
and logs at every one of these checkpoints so that we 
can-uh proper evaluate this adventure uh the second one is 
the evaluation-guideline and this felt for me a lot like a 
an interview process uh I develop-creating a rubric uh of 
all of the of what should the model achieve like what-do 
you take is a a good model uh on which each criteria what 
is good what is-okay what is not acceptable so for example 
it's not acceptable to for the-model to have toxicity High 
toxicity what is high-uh toxicity what is the Benchmark 
that you're going to use to test that-out and then tie this 
evaluation possibly to business Matrix because if-you can 
TI to business matrix it's much easier to explain to 
business in general-and other folks how changes in metrics 
impact real world-yeah usage of of tool so for example 
toxicity it might decrease uh it might-cause an epti or 
funny Twitter uh images of du but it might cause problems 
and-legal issues later so this was uh and the quote is is 
this was before already-on machine learning we used to say 
that the hardest part was measuring and-knowing what to 
build in the first place rather than building the 
machine-learning model but it's the same like the hardest 
part easily just defining-how to evaluate and we delayed 
this quite a lot-because we're building uh stuff and now we 
are going back and doing proper evalu-trying to do proper 
evaluation of the things that we-built and learn along the 
way and this was for me the most-important part of the 
value the finding evaluation methods and-data um the the 
the the author mentions to automate but do not forget 
about-human evaluation and I think this is really really 
important with AI often-the automations would build hide 
actual results-so especially before going GA or I don't 
know on some certain C some Cadence we-need to have human 
evaluators looking at the data and and processing it it 
is-there I forgot to link here but I read a paper that they 
show that there is there-is is often a Revere relationship 
between automated results and human-perception meaning a 
change in the model improves-the automated response but the 
humans like don't like it as much and we are-catering to 
humans for now so that's uh that's what where we should 
focus-on um log props these are what is the likelihood of 
the next prediction this-can give a good result good idea 
on how our model is performing or to avoid h-ination for 
example Hallucination is a best case uh the the model 
didn't know-how to fill it and they gave something but 
probably that something is a low uh-probability of that 
should have appeared so log prop could be used for that 
as-well put effort on annotating data so build your own 
data set uh get like-write down get examples and write down 
what it should be or uh write down how-the the response 
should look like so that you can have like a 
proper-evaluation harness behind it uh so that's uh that's 
important also L the-data like you're you should be 
evaluating across all of your target-groups of users and 
for example our self-hosted customers might-behave 
differently than our customers their desires like their the 
way they-use the tool might be different if we evaluate 
only against our Doom-customers uh it might be hiding 
what's happened on on our selfhosted so we need-to balance 
that out and figure out how to do-that uh and the one is 
constantly evaluate your pipeline we should I think-we we 
should be able to answer all all of these five questions 
about our own-pipelines is your evaluation getting the 
right signal if you improve this is-really important if you 
actually improve uh your responses do they mean 
higher-scores and in the verse also if we if you get higher 
scores do they actually-mean an improved response because 
if that doesn't happen why are we even-evaluating like if 
the if you cannot make a decision on your evaluation 
like-what's the point of evaluating in the first-place um 
how reliable is your evaluation pipeline if you run 
multiple times you-should get very similar results it 
shouldn't be too different especially if-you um if you 
group uh different measurements uh how correlated are 
your-metrics if you have too many Matrix and they all go 
into the same direction they-will not tell you anything 
like different like if you have you should-have matric 
measuring different things and yeah how much cost 
and-latency this evaluation pipeline this is something that 
also bugs me even with-the gitlab tests for example like we 
have this huge test uh-pipeline that is that runs on every 
commit on every merch that wastes a lot-of resources um and 
it says this will be even worse here like if you want 
to-evaluate all the time imagine the M the amount of 
resource you're going to waste-just by pring the the 
evaluation and yeah that was my-summary of this chapter for 
me I try to summarize quite but this chapter was-really 
interesting I really enjoy it-comments thanks for the 
summary uh definitely um summarize that well I-think one 
thing that this chapter missed for me was something around 
um diversity-of your data that you're using to evaluate um 
think that like I'm I-noticed with our code generation 
evaluation is everything is a python-single line method and 
um we have really high scores but does it actually 
mean-anything um if we have customers with bash and 
customers with you know-different languages and um even 
just yaml you know that's completely-different to a single 
line method um yeah you definitely need to strive to-get 
some kind of a diversity in your data that you're using to 
evaluate your-metrics so um yeah thought that was 
interesting and those single end methods-are likely on the 
training data set of the model as well so you have 
data-contamination and the model might have memorized and 
just like interpolate a-little bit over there but it's 
memorized that's another problem-yeah um I think yeah I was 
surprised when I first looked at the evaluation-metrics I 
was like wow it's we're doing really good and then I saw um 
the data-set that be using and I thought yeah that's-why 
yeah I think data set is already something uh they are 
working on because-we need a better data set a more diverse 
data set they were mentioning on the-book 300 uh data 
points I think we have 45s I I don't remember remember 
the-number but as much as we have we can use our own code 
base to generate those data-sets but that also a little bit 
of a difference and our data set our codebase-is already um 
indexed as well by these-models because it's public data so 
they're like they-already some memorization yeah and it's 
interesting-um just for I don't know if everyone knows this 
but the metrics that we-measure for the code generation 
byeline is correctness comprehensiveness and-treatability 
um the book couple of metrics and it was um interesting I 
mean-correctness is basically like the functional 
correctors that they they-talked about um comprehensiveness 
I'm not actually sure-how we measure that um maybe it's 
around um looking at the-code before and after the cursor 
to see if it fits into if the Cod generated-fits into that 
I'm not sure how we measure that um um and then 
readability-the book mentioned how to measure for that so 
yeah I think the use a as a-judge even for the correctness 
I'm not entirely sure if they go through uh like-actual 
functional of going through a test but it could be an-idea 
but I think it's a as a judge right yeah it's a it's a 
judge LM judge-who actually uh send the score it's kind of 
the same for chat it's also-correctness reability and 
comprehensiveness but funny enough for-all of our um 
dashboards and metrics and like reports we use only 
correctness as-a score mhm and we're close to four on 
those-scores right like you go between zero and four and 
always-close this was something that came up recently on my 
team when we were looking-at changing the prompt to use um 
X-ray and like some of the rag stuff that we-had developed 
to change it to see if any changes would have a difference 
in the-responses that were generated so we needed to have a 
data set to be able to-actually evaluate that and it was 
super hard to come up with because how do you-determine if 
the extra context you're feeding is actually providing 
better-responses or not um so then yeah we ended up using 
an llm as a judge but-then it was the same llm so was 
anthropic evaluating anthropic and-there's that 
confirmation bias kind of feedback loop but um but it still 
showed-like an improvement in in some of the things that we 
added to the prompt which-was interesting um but then it 
was also a very manual process like it wasn't a-data set 
that was scalable to become thousands of entry points it 
was-something that someone had to review every single entry 
point and modify to-kind of help mitigate that anthropic 
bias having generated the example um and-being the judge um 
so yeah I I don't I would be curious because this 
is-something that's going to come up again of course like 
as taana knows I'm-working on the AI context project now 
where we're trying to feed even more-context and pinned 
context into our prompts and things um so like what 
data-set are going to be able to use to evaluate whether 
chat is using that-extra context is it better is it not and 
how does that scale to become something-that you can just 
easily iterate on which I didn't I didn't feel the-project 
the the chapter gave too many concrete examples of how to 
do other-than using like existing benchmarks and things or 
generating your own-examples I think this is this could be 
a good idea for another-uh another Reading Club how to 
create data sets and if there's something I-think that 
would be very interesting I know that AI framework uh and 
AI-validation team mod validation teams are working on some 
guidelines to achieve-that but we can search and help them 
out in this as well but I think that would-be very 
interesting to see like how do we create a data set of the 
right uh-Dynamic content with and without we had this 
problem like on a simple thing I-was implementing uh 
context uh documentation search for self posted a-few uh 
weeks ago and it was hard to evaluate if the-search was 
good because the model never requested-search our gitlab 
data is index our documentation index so it was 
just-pulling from uh directly from the the model itself it 
didn't need the rag so-how do I force that and in some 
cases for simple questions it was requesting-the the the 
documentation and that threw off thear-because the it none 
of the documentation related to the question original but 
it-added more tokens to the to the prompt that confused llm 
so it's always that-it's a it's a good thing but and 
especially on this case like-okay context might improve but 
which context like how the the the users would-generate I 
think this answer will be better if you one way we could 
generate-this data is ux research uh when you're doing 
ux-research we record the context that they are using and 
they are adding into the-into the documentation or what is 
appearing h and use that context as orig-some source of 
data set or pay use like pay users to use the tool we like 
some-bash uh type of thing and then get the data as a 
source for our uh-tests yeah kind struck me is oh go ahead 
sorry um something that struck me is it-seems like mult 
teams have a need to evaluate for different metrics 
and-different things so the only thing I was interested in 
was the rag capability was-the change actually improving 
what happened but I didn't care much about-cost or latency 
or like this was the one thing that I wanted to improve um 
so I-wonder how that overlaps like should the evaluation 
pipeline be-inclusive of everything like someone who's only 
updating a prompt should they-also be looking at all of the 
other metrics or is that okay to divide so one-team might 
look at some metrics while another team focuses on other 
things um-and like how how should we as git laabs think on 
that to make sure that we're-actually covering the entire 
pipeline I-think we you should have a bird's eye view of 
what's Happening like your-changes might in like you're 
look at latency like you you might want to-create a like no 
functional changes that will bring down to that happens uh 
like-without even with our best intentions but I like for I 
don't want for example-Cod the global search team that is 
working on R to care about changes that-are way ahead in 
the prompt like if something breaks on the prompt we 
should-know that that team is resp like it's on the part 
that one specific team is-responsible right and this is why 
we need-the the kind of like divided our attributed way of 
measuring impact so-how does rag is how is rag impacting 
the the or not rag but how is each specific-rag impacting 
the the the usage how is the prompt impacting the usage how 
is-the model impacted impacting the final metrix how is the 
rest impacting because-there's like text processing happen 
in the middle of the of the whole thing uh-on gitlab side 
that needs to be part of it as well um so it's true I don't 
think-we should be able we should consider this part of 
like very independent-Parts but each team should be able to 
like split out what is their impact and-what is the rest 
right yeah agreed and then also I agree-with Missy that we 
need a way to quickly iterate and like make a change run 
it-through the evaluation see what is better what is worse 
so that you don't-spend time like running everything but uh 
you get this Fe faster feedback loop-in your development 
cycle um kind of like a unit test tells you quickly 
like-this is breaking or this is passing you know um and 
then once you push it to the-branch then it runs all of 
these other metrics and pipelines to say like the-overall 
system is impacted or not um so you also need like-yeah 
development practices basically um faster and slower 
feedback-loops yeah I think they will the book will-cover 
like in production um monitoring much later I think it's 
chapter 8 which-is currently not available only until 
chapter-5 um and that will also help with this-iteration if 
I push something does it change anything because sometimes 
we-cannot see the difference on such small amount of data 
that we have but when we-push to customers and you have 
large uh usage then we can see then we have 
more-statistical power to Define like what what happened-uh 
it's sometimes like even what I said of like let's remove 
Rag and see the-impact we don't see anything it's not that 
impact is not there it might not be-there but but it's more 
likely that the amount of data we have is not able 
to-capture that or the type of data or the type of queries 
don't are not beneficial-for them so there are a lot of 
different Compounders last coms yes yeah one-comment that I 
had something that I had never considered before after 
before-reading this chapter was um in the decision of 
chosing to buy a model or-build your own model and then if 
you end up using somebody else's model as a-service then 
there's a choice of using a public open source one or a 
private one-um so I never I never thought about the 
implications of what that meant for a-company especially 
gitlab given that we're open core um having chosen 
a-private model so yeah I thought that was that was an 
added layer to think about-because it's kind of a black box 
we don't know what model is giving us what-data it was 
train on we don't know all of the input to it but then 
everything-we do is so open and out like for anyone to be 
able to dissect and see so it's-kind of like yeah it's both 
sides of I guess the the open source debate that I-hadn't 
considered and I I thought that was an interesting point in 
the-book used to have an effort to create our own model but 
gradi model is-expensive and it's not that's most the 
reason uh which go-different uh but yeah a lot of our 
customers are wanting to deploy their-own models because 
they want have full control and this what they want 
they-have no choice their their regulated systems require 
them to run like-everything behind a behind firewall so it 
is what it-is but it's it's always ironic that we use open 
like a closed model like this-with be Le an open uh company 
like we are I imagine we were kind of forced-into it though 
like the book says that in general the open source models 
tend-to be less performant than the proprietary ones and so 
in order to-remain competitive and iterate faster we're 
kind of obligated to use something-that's competitive in 
the Market at the time but kind of a-bummer and we also had 
to choose between time invested in deploying a-motel and 
hosting a motel and time into deploying features we did 
less work-on we would have to have done a lot more work on 
infrastructure we were to deploy-our own models than 
currently even though we put a lot of effort 
in-infastructure right even with a host API a lot of 
infrastructure is-necessary the funny thing is that seual 
about prary model uh when we were-releasing chat as a ga we 
have some uh concerns from users that oh like are you-going 
to train uh chat on our data and then when-we say like no 
this is actually kind of we don't this is like model by API 
we-don't kind of we can't train it so they were kind of 
quite um happy about-that kind of like PR and cons as well 
we can guarantee this later as well-especially because 
we're open for they can look at the code and see what we 
are-using for tuning or training whatever we might want to 
train eventually like-special self hosted we might find tun 
their own date but it's within their own-environment and 
opting so if they want to have like a better model 
for-something and we explore fine tuning we could have fine 
tuning happening on-selfhosted so they want to do so they 
still they still own their data the data-doesn't come to us 
it's closed but we have that support going-awesome cool so 
thank you all for joining uh if you have any more-questions 
to discuss uh we can move to the issue later um I found 
this chapter-quite uh exciting and difficult uh but at the 
same time gave a lot of ideas to-think about so it was a 
good one thank you all for joining um and next time-will be 
M's mie uh we will uh study a different thing and I will 
share the-content thank-you thanks thanks bye by bye
